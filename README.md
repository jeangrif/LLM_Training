## 1ï¸âƒ£ Overview

This repository implements a **Retrieval-Augmented Generation (RAG)** pipeline using **Hydra** for configuration management.

It provides:
- A **data preparation workflow** (download â†’ preprocessing â†’ augmentation)
- A **Hydra-driven modular pipeline** for model inference or training
- A clean, configurable structure for reproducible experiments
## 2ï¸âƒ£ Installation

This project uses **[uv](https://github.com/astral-sh/uv)** to manage Python environments efficiently.

```bash
# Create and synchronize the environment
uv sync

# Activate the environment
source .venv/bin/activate  # (or .venv\Scripts\activate on Windows)
```
If you prefer to install manually with pip:
```bash
pip install -r requirements.txt
```
## 3ï¸âƒ£ Project Structure

```bash
project_root/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                # Hydra pipeline entrypoint
â”‚   â””â”€â”€ rag/                   # Core RAG logic and modules
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingest_hf_dataset.py   # Step 1 â€“ Download & save HF dataset as Parquet
â”‚   â”œâ”€â”€ build_eval_base.py     # Step 2 â€“ Extract base Q/A pairs for evaluation
â”‚   â”œâ”€â”€ generate_paraphrases.py # Step 3 â€“ Create paraphrased question variations
â”‚   â””â”€â”€ prepare_data.py        # Orchestrates all 3 data preparation steps
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Raw Parquet dataset
â”‚   â””â”€â”€ eval/                  # Base + augmented evaluation sets
â”‚
â”œâ”€â”€ index/                #  FAISS / vector index
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ pipeline.yaml          # Hydra config for the main pipeline
â”‚
â”œâ”€â”€ pyproject.toml / uv.lock   # Dependencies
â””â”€â”€ README.md
```
## 4ï¸âƒ£ Data Preparation

Before running the Hydra pipeline, you must first prepare the dataset.  
This step downloads a dataset from **Hugging Face**, extracts **questionâ€“answer pairs**, and generates **paraphrased variations** for evaluation.

### ğŸ”¹ Run the full preparation process
```bash
python scripts/prepare_data.py
```
This command automatically executes the three preparation steps in sequence:

| Step | Script | Output |
|------|---------|--------|
| 1ï¸âƒ£ Ingest dataset | `scripts/ingest_hf_dataset.py` | `data/raw/<dataset>_<split>.parquet` |
| 2ï¸âƒ£ Build base eval | `scripts/build_eval_base.py` | `data/eval/base_questions.jsonl` |
| 3ï¸âƒ£ Generate paraphrases | `scripts/generate_paraphrases.py` | `data/eval/augmented_questions.jsonl` |

Example output:

âœ… data/raw/squad_train.parquet  
âœ… data/eval/base_questions.jsonl  
âœ… data/eval/augmented_questions.jsonl
## 5ï¸âƒ£ Configuration for Data Preparation

Each script inside the `scripts/` folder contains a small configuration block named `CONFIG` at the very top of the file.  
This block centralizes all editable parameters â€” you can modify dataset names, paths, or model options directly here.

### ğŸ”¹ Example â€“ `scripts/ingest_hf_dataset.py`
```bash
CONFIG = {
    "hf_dataset_name": "squad",
    "hf_dataset_split": "train",
    "text_field": "context",
    "raw_dir": Path("data/raw"),
}

### ğŸ”¹ Example â€“ `scripts/generate_paraphrases.py`

CONFIG = {
    "model_name": "eugenesiow/bart-paraphrase",
    "input_path": Path("data/eval/base_questions.jsonl"),
    "output_path": Path("data/eval/augmented_questions.jsonl"),
    "limit": 200,
    "device_preference": "auto",
}
```
ğŸ‘‰ No `.env` file or external configuration is required.  
All settings are explicit, versioned, and easy to track directly inside each script.
## 6ï¸âƒ£ Running the Hydra Pipeline

Once the dataset is prepared, you can launch the main pipeline directly.

### ğŸ”¹ Default run
```bash
python -m src.main
```
Hydra will automatically handle:
- Output directory creation under `outputs/`
- Logging (`main.log`)
- Configuration management through YAML files
- Modular execution of each stage (Preprocessing, Model, Evaluation, etc.)
## 7ï¸âƒ£ Modifying the Pipeline Configuration

All parameters are defined in the `configs/` directory.  
Edit `pipeline.yaml` or the submodules (e.g. `llm/settings.yaml`, `embed/settings.yaml`, `model/llama_cpp.yaml`) to adjust models, paths, or modes before running `python -m src.main`.
7ï¸âƒ£ Modifying the Pipeline Configuration

----------------------------------------

All parameters are defined in the configs/ directory.The main file configs/pipeline.yaml defines the stages and modules of the pipeline.

### Example structure

* Â  **defaults** : lists all submodules (model, llm, embed, rerank, eval, latency)

* Â  **stages** : defines the execution order (check\_models, check\_data, run\_rag, evaluate)

### Stage overview

* Â  **check\_models** â†’ verifies or downloads all required models

* Â  **check\_data** â†’ checks or builds the FAISS index and embeddings

* Â  **run\_rag** â†’ runs the main RAG process on the augmented questions

* Â  **evaluate** â†’ computes and saves RAG performance metrics

You can modify any parameter (model paths, chunk size, overlap, top\_k, etc.) directly in configs/pipeline.yaml before running:python -m src.main