> âš ï¸ **Note**  
> This README was partially generated and refined with the help of an LLM (ChatGPT).  
> Always verify configuration paths and environment variables before running the pipeline.

## 1ï¸âƒ£ Overview

This repository implements a **Retrieval-Augmented Generation (RAG)** pipeline using **Hydra** for configuration management.

It provides:
- A **data preparation workflow** (download â†’ preprocessing â†’ augmentation)
- A **Hydra-driven modular pipeline** for model inference or training
- A clean, configurable structure for reproducible experiments
## 2ï¸âƒ£ Installation

This project uses **[uv](https://github.com/astral-sh/uv)** to manage Python environments efficiently.

```bash
# Create and synchronize the environment
uv sync

# Activate the environment
source .venv/bin/activate  # (or .venv\Scripts\activate on Windows)
```
If you prefer to install manually with pip:
```bash
pip install -r requirements.txt
```
## 3ï¸âƒ£ Project Structure Overview

```text
src/
â”œâ”€â”€ components/                    # Core building blocks used by stages/pipeline
â”‚   â”œâ”€â”€ embed/                     # Embedding backends & helpers
â”‚   â”œâ”€â”€ llm/                       # LLM backends (base classes, llama.cpp, etc.)
â”‚   â”œâ”€â”€ retriever/                 # Retrievers (dense FAISS, BM25, hybridâ€¦)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ generator.py               # LLM selection/wrapper exposing a unified `generate()`
â”‚   â””â”€â”€ reranker.py                # (Optional) re-ranking of retrieved passages
â”‚
â”œâ”€â”€ eval/                          # Evaluation logic (quality) & system performance
â”‚   â”œâ”€â”€ metrics/                   # Exact match, semantic similarity, lexical overlap, faithfulnessâ€¦
â”‚   â”œâ”€â”€ performance/               # Latency meter / runtime KPIs
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ optimization/
â”‚   â””â”€â”€ optuna_search.py           # Optuna study for hyperparameter tuning
â”‚
â”œâ”€â”€ pipeline/
â”‚   â””â”€â”€ orchestrator.py            # Orchestrates the dynamic stage flow (Hydra-driven)
â”‚
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ engine.py                # RAG core: retrieve â†’ (rerank) â†’ generate for one query
â”‚
â”œâ”€â”€ scripts/                       # Ad-hoc utilities / one-off scripts (placeholder)
â”‚
â”œâ”€â”€ stages/                        # Executable units run by the orchestrator (sequentially)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ check_models.py            # Verify/download required models
â”‚   â”œâ”€â”€ index_manager.py           # Build/load search indexes
â”‚   â”œâ”€â”€ rag_runner.py                  # Batch RAG run over dataset (uses rag.pipeline)
â”‚   â””â”€â”€ evaluator.py               # Compute evaluation metrics & persist results
â”‚
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ app.py                     # Minimal app/CLI/UI to drive the pipeline
â”‚
â”œâ”€â”€ utils/
â”‚
â””â”€â”€ main.py                        #entry point                      
```
## 4ï¸âƒ£ Data Preparation

Before running the Hydra pipeline, you must first prepare the dataset.  
This step downloads a dataset from **Hugging Face**, extracts **questionâ€“answer pairs**, and generates **paraphrased variations** for evaluation.

### ğŸ”¹ Run the full preparation process
```bash
python src/scripts/prepare_data.py
```
This command automatically executes the three preparation steps in sequence:

| Step | Script | Output |
|------|---------|--------|
| 1ï¸âƒ£ Ingest dataset | `scripts/ingest_hf_dataset.py` | `data/raw/<dataset>_<split>.parquet` |
| 2ï¸âƒ£ Build base eval | `scripts/build_eval_base.py` | `data/eval/base_questions.jsonl` |
| 3ï¸âƒ£ Generate paraphrases | `scripts/generate_paraphrases.py` | `data/eval/augmented_questions.jsonl` |

Example output:

âœ… data/raw/squad_train.parquet  
âœ… data/eval/base_questions.jsonl  
âœ… data/eval/augmented_questions.jsonl
## 5ï¸âƒ£ Configuration for Data Preparation

Each script inside the `scripts/` folder contains a small configuration block named `CONFIG` at the very top of the file.  
This block centralizes all editable parameters â€” you can modify dataset names, paths, or model options directly here.

### ğŸ”¹ Example â€“ `scripts/ingest_hf_dataset.py`
```bash
CONFIG = {
    "hf_dataset_name": "squad",
    "hf_dataset_split": "train",
    "text_field": "context",
    "raw_dir": Path("data/raw"),
}
```
### ğŸ”¹ Example â€“ `scripts/generate_paraphrases.py`
```bash

CONFIG = {
    "model_name": "eugenesiow/bart-paraphrase",
    "input_path": Path("data/eval/base_questions.jsonl"),
    "output_path": Path("data/eval/augmented_questions.jsonl"),
    "limit": 200,
    "device_preference": "auto",
}
```
ğŸ‘‰ No `.env` file or external configuration is required.  
All settings are explicit, versioned, and easy to track directly inside each script.
## 6ï¸âƒ£ Running the Hydra Pipeline

Once the dataset is prepared, you can launch the main pipeline directly.

### ğŸ”¹ Default run
```bash
python -m src.main
```
Hydra will automatically handle:
- Output directory creation under `outputs/`
- Logging (`main.log`)
- Configuration management through YAML files
- Modular execution of each stage (Preprocessing, Model, Evaluation, etc.)
## 7ï¸âƒ£ Modifying the Pipeline Configuration

All parameters are defined in the `configs/` directory.  
Edit `pipeline.yaml` or the submodules (e.g. `llm/settings.yaml`, `embed/settings.yaml`, `model/llama_cpp.yaml`) to adjust models, paths, or modes before running `python -m src.main`.
7ï¸âƒ£ Modifying the Pipeline Configuration

----------------------------------------

All parameters are defined in the configs/ directory.The main file configs/pipeline.yaml defines the stages and modules of the pipeline.

### Example structure

* Â  **defaults** : lists all submodules (model, llm, embed, rerank, eval, latency)

* Â  **stages** : defines the execution order (check\_models, check\_data, run\_rag, evaluate)

### Stage overview

* Â  **check\_models** â†’ verifies or downloads all required models

* Â  **check\_data** â†’ checks or builds the FAISS index and embeddings

* Â  **run\_rag** â†’ runs the main RAG process on the augmented questions

* Â  **evaluate** â†’ computes and saves RAG performance metrics

You can modify any parameter (model paths, chunk size, overlap, top\_k, etc.) directly in configs/pipeline.yaml before running:python -m src.main

## ğŸ§­ Next Steps â€” RAG Pipeline Improvements

| # | Category | Task | Status |
|:-:|-----------|-------|:------:|
| **1** | ğŸ—‚ï¸ **Improve folder structure** |  | â˜ |
| **2** | âš™ï¸ **Improve config file** |  | â˜ |
| **3** | ğŸ§  **Improve Rerank module** |  | â˜ |
| **4** | ğŸ§ª **Continue Optuna script** |  | â˜ |
| **5** | ğŸ§© **Improve App.py** |  | â˜ |
