# src/ui/app.py
import sys
from pathlib import Path
import pandas as pd
# --- Chemin racine du projet ---
ROOT = Path(__file__).resolve().parents[2]
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))
import traceback
import streamlit as st
from hydra import compose, initialize
from omegaconf import OmegaConf
from src.rag.engine import RagPipeline
from src.db.connection import get_db_connection
from src.db.logger import RagLogger




def main():
    st.set_page_config(page_title="Chat RAG (Hydra)", layout="wide")
    st.title("üß† Chat RAG")

    # -----------------------------
    # ‚öôÔ∏è Chargement automatique de la config Hydra
    # -----------------------------
    with initialize(config_path="../../configs", version_base=None):
        cfg = compose(config_name="pipeline")

    rag_cfg = cfg.modules.run_rag
    try:
        logger = RagLogger()
        st.sidebar.success("üìó Logger initialis√© avec succ√®s !")
    except Exception as e:
        logger = None
        st.sidebar.error(f"‚ùå Erreur lors de l'initialisation du logger : {e}")
        st.sidebar.code(traceback.format_exc())



    # -----------------------------
    # ‚öôÔ∏è Sidebar interactive
    # -----------------------------
    st.sidebar.header("‚öôÔ∏è Options")

    retrieval_type = st.sidebar.selectbox(
        "Retrieval type", ["dense", "sparse", "hybrid"],
        index=["dense", "sparse", "hybrid"].index(rag_cfg.retrieval_type)
    )
    top_k = st.sidebar.slider("Top K", 1,32, rag_cfg.top_k)
    use_rerank = st.sidebar.toggle("Activer le rerank", rag_cfg.use_rerank)
    top_k_rerank = st.sidebar.slider("Top K Rerank", 1, 10, rag_cfg.top_k_rerank)
    alpha = st.sidebar.slider("Alpha (poids hybrid)", 0.0, 1.0, rag_cfg.alpha)
    stateful_mode = st.sidebar.toggle("üß† Mode conversation (stateful)", value=True)

    embedding_model = st.sidebar.text_input("Embedding model", rag_cfg.embedding_model)
    model_meta = {
        "llm_repo": cfg.llm.llm_repo,
        "llm_path": str(Path(cfg.llm.local_dir) / cfg.llm.llm_filename),
        "chat_format": cfg.llm.get("chat_format", "mistral-instruct"),
    }
    if st.sidebar.button("üîÑ Reset conversation"):
        if "rag" in st.session_state:
            try:
                rag = st.session_state.rag
                if hasattr(rag, "reset_context"):
                    rag.reset_context()  # vide le contexte interne du mod√®le
            except Exception as e:
                st.sidebar.warning(f"Erreur lors du reset du mod√®le : {e}")
        st.session_state.clear()
        st.toast("Contexte r√©initialis√© üßπ")
    st.sidebar.divider()


    if logger is not None:
        if "model_id" not in st.session_state:
            try:
                st.session_state.model_id = logger.log_model_config({
                    "retrieval_type": retrieval_type,
                    "top_k": top_k,
                    "use_rerank": use_rerank,
                    "alpha": alpha,
                    "embedding_model": embedding_model,
                    "model_meta": {
                        "llm_repo": cfg.llm.llm_repo,
                        "llm_path": str(Path(cfg.llm.local_dir) / cfg.llm.llm_filename),
                        "chat_format": cfg.llm.get("chat_format", "mistral-instruct"),
                    },
                })
                st.sidebar.success(f"üßæ Config logg√©e (ID {st.session_state.model_id})")
            except Exception as e:
                st.sidebar.error(f"‚ùå Erreursss lors du log de la config : {e}")

    # -----------------------------
    # ‚öôÔ∏è Initialisation du pipeline RAG (une seule fois)
    # -----------------------------
    if "rag" not in st.session_state:
        # üîπ Reconstruire dynamiquement le chemin de l'index √† partir de la config Hydra
        index_base = Path(cfg.embed.index_dir)
        embed_model = cfg.embed.embedding_model.replace("/", "-")
        chunk = cfg.embed.chunk_size
        overlap = cfg.embed.chunk_overlap
        index_dir = index_base / f"{embed_model}__chunk{chunk}_ov{overlap}"

        # V√©rification rapide
        if not index_dir.exists():
            st.warning(f"‚ö†Ô∏è Index introuvable √† {index_dir}. V√©rifie que l‚Äô√©tape check_data a bien √©t√© ex√©cut√©e.")
        else:
            st.sidebar.success(f"üì¶ Index trouv√© : {index_dir.name}")

        # üîπ Initialisation du pipeline RAG
        st.session_state.rag = RagPipeline(
            top_k=top_k,
            retrieval_type=retrieval_type,
            use_rerank=use_rerank,
            alpha=alpha,
            embedding_model=embedding_model,
            model_cfg=rag_cfg.model_cfg,
            latency_cfg=rag_cfg.latency_cfg,
            index_dir=index_dir,
            model_meta=model_meta,
            stateful=stateful_mode,
            top_k_rerank=top_k_rerank
        )

    rag = st.session_state.rag

    # -----------------------------
    # üí¨ Chat
    # -----------------------------
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    query = st.chat_input("Posez votre question‚Ä¶")

    if query:
        st.session_state.messages.append({"role": "user", "content": query})
        with st.chat_message("user"):
            st.markdown(query)

        with st.spinner("ü§ñ G√©n√©ration en cours..."):
            try:
                result = rag.run(query)
                answer = result["pred"]
                contexts = result["contexts"]
                latency_summary = None
                if hasattr(rag, "latency_meter") and rag.latency_meter:
                    latency_summary = rag.latency_meter.summary()
                else:
                    latency_summary = None
            except Exception as e:
                answer = f"‚ö†Ô∏è Erreur : {e}"
                contexts = []
                latency_summary = None

        with st.chat_message("assistant"):
            st.markdown(answer)
        st.session_state.messages.append({"role": "assistant", "content": answer})

        if logger is not None and "model_id" in st.session_state:
            try:
                # üîπ Historique interne du mod√®le (cot√© LlamaCppProvider)
                model_messages = getattr(rag.generator.model, "messages", [])

                logger.log_interaction(
                    model_id=st.session_state.model_id,
                    query=query,
                    answer=answer,
                    contexts=contexts,
                    latency=latency_summary,
                    stateful=st.session_state.get("stateful_mode", False),
                    conversation_context=model_messages,
                )
                st.toast("üíæ Interaction logg√©e avec contexte !", icon="üí¨")
            except Exception as e:
                st.sidebar.error(f"‚ö†Ô∏è Erreur lors du log de l'interaction : {e}")

        if contexts:
            st.divider()
            st.subheader("üìö Contextes r√©cup√©r√©s")
            for i, ctx in enumerate(contexts, 1):
                with st.expander(f"Context {i}"):
                    st.write(ctx)


if __name__ == "__main__":
    main()
