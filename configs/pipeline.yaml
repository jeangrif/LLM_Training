# configs/pipeline.yaml
output_dir: outputs/default
defaults:
  - model: llama_cpp
  - llm: settings
  - embed: settings
  - rerank: settings
  - eval/metrics
  - latency: latency
  - _self_

stages:
  - check_models
  - check_data
  - run_rag
  - evaluate

modules:
  check_models:
    _target_: src.stages.check_models.CheckModels
    embed_model: ${embed.embedding_model}
    llm_repo: ${llm.llm_repo}
    llm_filename: ${llm.llm_filename}
    rerank_model: ${rerank.model_name}

  check_data:
    _target_: src.stages.index_manager.IndexManager
    embed_settings: ${embed}
    parquet_path: data/raw/squad_train.parquet

  run_rag:
    _target_: src.stages.rag_runner.RagRunner
    input_path: data/eval/initial_questions.jsonl
    output_path: outputs/results_rag.jsonl
    retrieval_type: hybrid  # dense, hybrid or sparse
    use_rerank: true  # true or false
    top_k: 4
    top_k_rerank: ${rerank.top_k}
    alpha: 0.6
    max_questions: 300
    index_dir: null
    embedding_model: ${embed.embedding_model}
    model_cfg: ${model}
    latency_cfg: ${latency}
    save_results: false
    propagate_keys: [index_dir]
    do_generation: false

  evaluate:
    _target_: src.stages.evaluator.Evaluator
    results_path: None
    output_path: outputs/eval_metrics.jsonl
    metrics: ${eval}
    embedding_model: ${embed.embedding_model}
    save_metrics: true
    propagate_keys: [qrels_path]
    do_generation: ${modules.run_rag.do_generation}

