# configs/pipeline.yaml
defaults:
  - model: llama_cpp
  - llm: settings
  - embed: settings
  - rerank: settings
  - eval/metrics
  - latency: latency
  - _self_

stages:
  - check_models
  - check_data
  - run_rag
  - evaluate

modules:
  check_models:
    _target_: src.rag.check_models.CheckModels
    embed_model: ${embed.embedding_model}
    llm_repo: ${llm.llm_repo}
    llm_filename: ${llm.llm_filename}
    rerank_model: ${rerank.model_name}

  check_data:
    _target_: src.rag.index_manager.IndexManager
    embed_settings: ${embed}
    parquet_path: data/raw/squad_train.parquet

  run_rag:
    _target_: src.rag.runner.RagRunner
    input_path: data/eval/augmented_questions.jsonl
    output_path: outputs/results_rag.jsonl
    retrieval_type: dense  # dense, hybrid or sparse
    use_rerank: true  # true or false
    top_k: 4
    alpha: 0.6
    max_questions: 50
    index_dir: null
    embedding_model: ${embed.embedding_model}
    model_cfg: ${model}
    latency_cfg: ${latency}
    save_results: false

  evaluate:
    _target_: src.eval.evaluator.Evaluator
    results_path: None
    output_path: outputs/eval_metrics.jsonl
    metrics: ${eval}
    embedding_model: ${embed.embedding_model}
    save_metrics: true

